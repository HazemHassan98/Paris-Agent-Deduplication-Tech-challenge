# ğŸ  Paris Agent Deduplication Challenge  

![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python&logoColor=white)  
![Pandas](https://img.shields.io/badge/Pandas-Data%20Analysis-orange?logo=pandas)  
![MongoDB](https://img.shields.io/badge/MongoDB-NoSQL-green?logo=mongodb)  

## ğŸ“Œ Overview  
This repository contains a solution for the **Paris Agent Deduplication Challenge**. The goal is to **clean and deduplicate** a dataset of real estate agents sourced from multiple platforms in Paris.  

The dataset, stored in **MongoDB**, consists of **nested documents** with various attributes such as:  
- **Agent Name**  
- **Phone Numbers**  
- **Email Addresses**  
- **Company Names**  
- **Addresses**  

## ğŸ“‘ Table of Contents  
- [Methodology](#methodology)  
- [Assumptions Made](#assumptions-made)  
- [Data Quality Issues](#data-quality-issues)  
- [Recommendations](#recommendations)  
- [Conclusion](#conclusion)  
- [Installation & Usage](#installation--usage)  
- [Dependencies](#dependencies)  
- [Contributors](#contributors)  
- [License](#license)  

## ğŸ›  Methodology  

We explored **four different approaches** for deduplication:  

### 1ï¸âƒ£ **Pandas `drop_duplicates()` Method**  
- **Approach**: Flatten nested documents into columns and apply Pandas' `drop_duplicates()` function.  
- **Limitation**:  
  - Struggles with high-dimensional datasets due to excessive column expansion.  
  - Not ideal for deeply nested structures.  

### 2ï¸âƒ£ **MongoDB `ensureIndex()` Method**  
- **Approach**:  
  - Create a **unique compound index** on key fields (**Agent name, Phone numbers, Email addresses**) using MongoDB's **`ensureIndex()`** with `dropDups=True`.  
- **Advantage**:  
  - Utilizes **MongoDBâ€™s indexing mechanism** to drop duplicates automatically.  
- **Note**:  
  - `dropDups` is **deprecated in MongoDB 3.0+** and is used here for **demonstration purposes only**.  

### 3ï¸âƒ£ **Hashing Approach**  
- **Approach**: Generate **hash values** for relevant fields. If two records share the same hash, one is removed.  
- **Advantage**:  
  - **Fast and efficient** for large datasets.  
- **Limitation**:  
  - Sensitive to minor variations (e.g., extra spaces, punctuation).  

### 4ï¸âƒ£ **Fuzzy Matching with RapidFuzz**  
- **Approach**:  
  - Use **RapidFuzz** to compute similarity scores for text fields (e.g., agent names).  
  - If **similarity score > 90%**, records are considered duplicates.  
- **Advantage**:  
  - Captures **near-duplicates** despite typos or formatting inconsistencies.  
- **Limitation**:  
  - Computationally expensive on large datasets.  

---

## ğŸ” Assumptions Made  
âœ… **Flattening Nested Data**: Used `pandas.json_normalize()` to extract key fields from **nested MongoDB documents**.  
âœ… **Uniformity in Data**: Assumed **cleaned fields** would provide a reliable basis for matching duplicates.  

---

## âš ï¸ Data Quality Issues  

### ğŸ”¹ **Completeness**  
- Several fields contain **missing or incomplete data**, affecting matching accuracy.  

### ğŸ”¹ **City Column Inconsistencies**  
- "Paris" appears in multiple formats:  
  - `"PARIS"` (uppercase)  
  - `"paris"` (lowercase)  
  - `"Paris, 75001"` (with postal code)  

### ğŸ”¹ **Address Anomalies**  
- Some records contain placeholders like **"------"** â†’ Should be treated as missing values.  

### ğŸ”¹ **General Inconsistencies**  
- **Punctuation, extra spaces, and formatting variations** complicate deduplication.  

---

## âœ… Recommendations  

### ğŸ”¹ **Enhanced Data Cleaning**  
ğŸ”¹ **Normalize city names and addresses** during data ingestion.  
ğŸ”¹ Replace **placeholder values ("------")** with **empty/null values**.  

### ğŸ”¹ **Metadata Collection**  
ğŸ”¹ Store **platform-specific identifiers** (e.g., **social media handles, unique IDs**) to improve duplicate detection.  

### ğŸ”¹ **Advanced Techniques**  
ğŸ”¹ Implement **machine learningâ€“based entity resolution** to handle ambiguous cases.  
ğŸ”¹ Use **parallel processing** to optimize performance for large datasets.  

### ğŸ”¹ **Evaluation Metrics**  
ğŸ”¹ Track **precision, recall, and F1-score** to assess deduplication effectiveness.  
ğŸ”¹ Use **visualizations** to monitor improvement over time.  

---

## ğŸ¯ Conclusion  
This project evaluates multiple deduplication techniques, each with **unique strengths and trade-offs**.  
- **Exact methods** (Pandas, MongoDB indexing, hashing) are **efficient but strict**.  
- **Fuzzy methods** (RapidFuzz) handle **near-duplicates** but can be **computationally intensive**.  

By **combining multiple techniques** and **enhancing data quality**, this solution provides a **scalable** approach to deduplication.  

---

## ğŸ’» Installation & Usage  

### ğŸ›  Prerequisites  
Ensure you have the following installed:  
- **Python (3.12)**
- **MongoDB**
- **Pandas**
- **RapidFuzz**

### ğŸ”¹ Clone the Repository  
```bash
git clone https://github.com/yourusername/paris-agent-deduplication.git
cd paris-agent-deduplication
